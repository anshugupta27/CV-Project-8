{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCSZR0K1wIfx"
   },
   "source": [
    "## **CIS 5810 Project 8-2 - 3D Hand Pose Estimation**\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In Part 1 you have seen how we can utilize the Transformer to build a simple yet effective hand pose estimation network. In part 2, we will extend the 3D hand pose estimation task to taking image as input using a new Transformer based network - [POTTER](https://github.com/zczcwh/POTTER/tree/main).\n",
    "\n",
    "**3D hand pose estimation from image**\n",
    "\n",
    "Recall in Part 1 where we are doing 2D-to-3D hand pose lifting by taking ground truth 2D hand joint coordinates as input, in this part we will take the image $I (H,W,3)$ as input (similar to Project 2) but we will predict the 3D hand pose $\\hat{P}(21,3)$ directly.\n",
    "\n",
    "\n",
    "**What to do**\n",
    "\n",
    "All parts that need your implementation are marked as *TODOs*, including model architecture build, training pipeline and final inference & evaluation. The file structure and helper function detail are listed below.\n",
    "\n",
    "- `imgs/`: directory where sample images for display are stored.\n",
    "- `dataset/`\n",
    "    - `dataset.py`: main Dataset to load and preprocess Ego-Exo4D data for model training.\n",
    "    - `dataset_vis.py`: some helper functions to visualize 3D hand kepypoints.\n",
    "- `utils/`\n",
    "    - `functions.py`: a list of utility functions to help model training and debugging.\n",
    "    - `loss.py`: implementation of loss function and metrics to evaluate model performance.\n",
    "- `model/`\n",
    "    - `potter.py`: Implementation of POTTER architecture and parts.\n",
    "    - `model.py`: Implementation of `PoolAttnHR_Pose_3D`, which is the model to do 3D hand pose estimation.\n",
    "- `CIS_5810_Project_8-2.ipynb`: intergrate every parts from above together from loading dataset to model training and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcFI-_7qwIf0"
   },
   "source": [
    "### 0 - Set-up\n",
    "\n",
    "Feel free to reuse the environment you set up in Project 3 Part 1. Otherwise, run `pip install -r requirement.txt` to install packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ublp-5b8wIf0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from dataset.dataset import ego4dDataset\n",
    "from model.model import load_pretrained_weights, PoolAttnHR_Pose_3D\n",
    "from tqdm import tqdm\n",
    "from utils.functions import (\n",
    "    AverageMeter,\n",
    "    update_config,\n",
    ")\n",
    "from dataset.dataset_vis import vis_data_3d\n",
    "from utils.loss import Pose3DLoss, mpjpe, p_mpjpe\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjNcA6H-wIf1"
   },
   "source": [
    "### 1 - Load Ego4D dataset\n",
    "\n",
    "Download annotation JSON files from [here](https://drive.google.com/drive/folders/1a_rhSuq5LsJQyiUVubQFYh8XGRwvMJnP?usp=sharing) and put it under `<anno_dir>`. The images to be used is the same as in Project 7, so feel free to re-use images from Project 7 by modifying `<img_dir>` to be the directory where you store the Project 7 images.\n",
    "\n",
    "Also, download the pretrained POTTER classification weight from [here](https://drive.google.com/file/d/14d8ky1d_oKXrZEqb3sM_atfpQM5Q6Ob0/view) for transfer learning, where we load in the pretrained classification weight first and then train on hand pose estimation. Modify the `potter_cls_weight` variable below to be the path of this weight stored on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jVvL4W2wIf2"
   },
   "outputs": [],
   "source": [
    "# TODO: Modify config as needed, e.g. annotation and image directory, training batch size etc.\n",
    "cfg = {\n",
    "        \"anno_dir\": ...,\n",
    "        \"img_dir\": ...,\n",
    "        \"model_cfg\": \"configs/potter_pose_3d_ego4d.yaml\",\n",
    "        \"potter_cls_weight\": ...,\n",
    "        \"lr\": 1e-4,\n",
    "        \"train_bs\": 16,\n",
    "        \"val_bs\": 16,\n",
    "        \"epochs\": 15,\n",
    "    }\n",
    "\n",
    "# Define the transform for image data preprocessing, which in here is just image normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# TODO: Initialize the train, val and test Dataset\n",
    "# Hint: take a look at the implementation of ego4dDataset to see how to initialize dataset\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "test_dataset = ...\n",
    "\n",
    "# Check the dataset length\n",
    "print(\"Train: \", len(train_dataset))\n",
    "print(\"Val: \", len(val_dataset))\n",
    "print(\"Test: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CYqtV8WwIf2"
   },
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "Take a look at the implementation of `dataset/dataset.py` which applies data preprocessing on both images and annotations, and returns four items when being indexed:\n",
    "\n",
    "- `input`: Cropped image from original Aria ego image s.t. hand is in image center.\n",
    "- `pose_3d_gt`: 3D hand keypoints in camera coordinate system, with hand wrist offseted and normalized (similiar to Part 1).\n",
    "- `vis_flag`: A boolean array indicating whether each hand joint has a valid 3D keypoint.\n",
    "- `metadata`: A dictionary consisting of current frame info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIQhkjvmwIf2"
   },
   "outputs": [],
   "source": [
    "## TODO: Modify as needed to take a look at the dataset\n",
    "check_dataset = train_dataset\n",
    "idx = 20000\n",
    "\n",
    "## Get one dataset sample for visualization\n",
    "input, pose_3d_gt, vis_flag, metadata = check_dataset[idx]\n",
    "\n",
    "## Visualization of input image and 3d kpts\n",
    "gt_3d = check_dataset.inv_normalize_3d(pose_3d_gt.numpy())\n",
    "# Assign None to invalid kpts (so it won't be displayed)\n",
    "gt_3d[~vis_flag] = None\n",
    "vis_data_3d(gt_3d, title=\"GT 3D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwILO6EXwIf3"
   },
   "source": [
    "### 2 - Define model\n",
    "\n",
    "**Generate Overview**\n",
    "\n",
    "POTTER (POoling aTtention TransformER) is a novel Transformer based architecture with the core design of Pooling Transformer Block (PAT), which replaces the original Attention block (Part 1) by Pooling Attention block. See Figure 1 for a comparison between different Transformer blocks.\n",
    "\n",
    "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"imgs/transformer_blocks.png\" alt=\"Different Transformer blocks\" width =\"450\" height=\"350\">\n",
    "        <figcaption>Figure 1: Different Transformer blocks</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "The overall model architecture of POTTER is shown in Figure 2, consisting of two stages: Basic stream and HR stream. The Basic stream has a hierachical structure with four stages, where the resolution of the feature map is gradually reduced to capture more global information. The global features from the basic stream are fused with the local features by patch split blocks in the HR stream. Finally, the output of stage 4 in HR stream are fed into the head block to predict the final pose information.\n",
    "\n",
    "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"imgs/POTTER_arch.png\" alt=\"Overall architecture of POTTER\" width =\"800\" height=\"310\">\n",
    "        <figcaption>Figure 2: Overall architecture of POTTER</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Pooling Attention in PAT Block**\n",
    "\n",
    "See Figure 3 for the details of each component in PAT block, which has a very similar structure as the original Transformer Encoder block you have implemented in Part 1.\n",
    "\n",
    "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"imgs/PAT.png\" alt=\"Pooling Attention Transfromer Block\" width =\"580\" height=\"330\">\n",
    "        <figcaption>Figure 3: Pooling Attention Transfromer Block (PAT)</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Given the input feature $X_{in}\\in \\R^{D \\times h \\times w}$, where $D$ is the embedding dimension and $h \\times w$ are the spatial dimension of the feature map, it is first normalized by Layer Norm as $X_0$, then passed to `PoolAttn` block to perform patch-wise pooling attention and embed-wise pooling attention. The output of `PoolAttn` block are then elementwisely added with $X_{in}$, which are then passed to FFN block to generate the final PAT block output $X_{out}$.\n",
    "\n",
    "- **Patch-wise Pooling Attention**\n",
    "\n",
    "    In patch-wise pooling attention, each patch's spatial locations are preserved while capturing the correlation between all patches. The input $X_0$ is first squeezed along $h$ axis and $w$ axis by two [adaptive average pooling](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html) to get $X_{Ph}$ and $X_{Pw}$, then the matrix multiplication between $X_{Ph}$ and $X_{Pw}$ gives to $X_1$.\n",
    "\n",
    "    $$X_{Ph} = Pool_1(X_0), \\quad X_{Ph}\\in \\R^{D\\times h \\times 4}$$\n",
    "    $$X_{Pw}  =Pool_2(X_0), \\quad X_{Pw}\\in \\R^{D \\times 4 \\times w}$$\n",
    "    $$X_1 = MatMul(X_{Ph}, X_{Pw}), \\quad X_1 \\in \\R^{D \\times h \\times w}$$\n",
    "\n",
    "- **Embed-wise Pooling Attention**\n",
    "\n",
    "    In embed-wise pooling attention, the similar spatial cross attention is performed along the embedding dimension. $X_0 \\in \\R^{D \\times h \\times w}$ is first reshaped to $X_{0}^{'} \\in \\R^{N \\times D_h \\times D_w}$, where $N = h\\times w$ and $D = D_h \\times D_w$. Then $X_{0}^{'}$ is squeezed along $D_h$ axis and $D_w$ axis by two adaptive average pooling to be $X_{PDh}$ and $X_{PDw}$. The matrix multiplication between $X_{PDh}$ and $X_{PDw}$ then gives to $X_2$, which are reshaped to $X_3\\in \\R^{D\\times h\\times w}$ with the same shape as $X_{0}$.\n",
    "\n",
    "    $$X_{PDh} = Pool_3(X_{0}^{'}), \\quad X_{PDh} \\in \\R^{N\\times D_h \\times 4}$$\n",
    "    $$X_{PDw} = Pool_4(X_{0}^{'}), \\quad X_{PDw} \\in \\R^{N\\times 4 \\times D_w}$$\n",
    "    $$X_2 = MatMul(X_{PDh}, X_{PDw}) \\quad X_2 \\in \\R^{N\\times D_h \\times D_w}$$\n",
    "\n",
    "The output from path-wise pooling and embed-wsie pooling are then projected by a convolutional layer separately, and added together with a layernorm and another convolutional layer as final projection to generate the `PoolAttn` blcok output.\n",
    "\n",
    "$$X_{out} = Proj_3(LN(Proj_0(X_1) + Proj_1(X_3))), \\quad X_{out} \\in \\R^{D\\times h \\times w}$$\n",
    "\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Your task is to finish the implementation of the Pooling Attention `PoolAttn` in PAT block, which are all marked as *TODOs* in `model/potter.py`. After your implementation, you can run cell below to perform simple check on model output tensor shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwcgoL3qwIf4"
   },
   "outputs": [],
   "source": [
    "# Simple Test\n",
    "model_cfg = update_config(cfg[\"model_cfg\"])\n",
    "model = PoolAttnHR_Pose_3D(**model_cfg.MODEL)\n",
    "\n",
    "input = torch.rand(1,3,224,224)\n",
    "output = model(input)\n",
    "assert output.shape == (1,21,3), \"Implementation is incorrect. Please check your PAT block\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B6nYvGxwIf4"
   },
   "source": [
    "### 3 - Train model\n",
    "\n",
    "Define the pipeline of model training and validation similar to Project 2. See below TODOs to fill in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI_Bh1sBwIf5"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    total_loss = AverageMeter()\n",
    "\n",
    "    # TODO: set model to training mode\n",
    "\n",
    "\n",
    "    train_loader = tqdm(train_loader, dynamic_ncols=True)\n",
    "    print_interval = len(train_loader) // 6\n",
    "    # Iterate over all training samples\n",
    "    for i, (input, pose_3d_gt, vis_flag, _) in enumerate(train_loader):\n",
    "        # TODO:\n",
    "        # 1. Put all revelant data onto same device\n",
    "        # 2. Model forward (given cropped hand image, predict a set of 3d kpts)\n",
    "\n",
    "\n",
    "        # TODO: Compute loss\n",
    "        loss = ...\n",
    "        total_loss.update(loss.item())\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Clear the old parameter gradients\n",
    "        # 2. Compute the derivative of loss w.r.t the model parameters\n",
    "        # 3. Update the model parameters with optimizer\n",
    "\n",
    "\n",
    "        # Log loss to wandb\n",
    "        if (i+1) % print_interval == 0:\n",
    "            wandb.log({\"Loss/train\": total_loss.avg})\n",
    "\n",
    "    # Return average training loss\n",
    "    return total_loss.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, device):\n",
    "    total_loss = AverageMeter()\n",
    "\n",
    "    # TODO: set model to evaluate mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loader = tqdm(val_loader, dynamic_ncols=True)\n",
    "        # Iterate over all validation samples\n",
    "        for i, (input, pose_3d_gt, vis_flag, _) in enumerate(val_loader):\n",
    "            # TODO:\n",
    "            # 1. Put all revelant data onto same device\n",
    "            # 2. Model forward (given cropped hand image, predict a set of 3d kpts)\n",
    "\n",
    "            # TODO: Compute loss\n",
    "            loss = ...\n",
    "            total_loss.update(loss.item())\n",
    "\n",
    "        # Log loss to wandb\n",
    "        wandb.log({\"Loss/val\": total_loss.avg})\n",
    "\n",
    "    # Return average training loss\n",
    "    return total_loss.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLc526PIwIf5"
   },
   "source": [
    "With the training and validation pipeline set up, we can then instantiate our model and define other essential parts for model training including:\n",
    "\n",
    "- **optimizer:** We will be using Adam optimizer with default learning rate 1e-4.\n",
    "\n",
    "- **loss function (criterion):** Use Pose3DLoss() as the loss function, which computes the MSE between predicted and ground truth 3D hand keypoints with filtering. The implementation has been given, please take a look at it before use.\n",
    "\n",
    "- **dataloaders**: train and val dataloader to return data in batches. Default batch size is 16, and you can adjust properly as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3ubDJ4jwIf5"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model and define device for training to use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_cfg = update_config(cfg[\"model_cfg\"])\n",
    "# Load in pretrained cls weight\n",
    "model = PoolAttnHR_Pose_3D(**model_cfg.MODEL).to(device)\n",
    "cls_weight = torch.load(cfg[\"potter_cls_weight\"])\n",
    "load_pretrained_weights(model.poolattnformer_pose.poolattn_cls, cls_weight)\n",
    "\n",
    "# TODO: Define loss function (criterion) and optimizer\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Define train and val dataloader\n",
    "train_loader = ...\n",
    "val_loader = ...\n",
    "\n",
    "# TODO: Define current run name\n",
    "current_run_name = time.strftime(\"%Y-%m-%d-%H-%M\") # Modify as needed, e.g. \"test_run_123\"\n",
    "wandb.init(project=\"CIS5810_project_8_2\", name=current_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zGb7yzOwIf9"
   },
   "source": [
    "Finally, we can start to train the model. We will log training and validation loss to wandb() to monitor the model training status. Run cell below to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqkoz2C5wIf-"
   },
   "outputs": [],
   "source": [
    "# Define output directory; modify as needed (where model ckpt will be saved)\n",
    "output_root = \"output\"\n",
    "output_dir = os.path.join(output_root, current_run_name)\n",
    "print(\"=\"*10 + f\" Training started. Output will be saved at {output_dir} \" + \"=\"*10)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Default training epoches and best val loss\n",
    "epochs = cfg[\"epochs\"]\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"=\"*10, f\"Epoch [{epoch}/{epochs}]\", \"=\"*10)\n",
    "    # train for one epoch\n",
    "    _ = train(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss = validate(val_loader, model, criterion, device)\n",
    "\n",
    "    # Save best model weight\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save model weight\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, f\"best_model_weight.pth.tar\"))\n",
    "        print(f\"Saving model weight with best val_loss={val_loss:.5f}\")\n",
    "    print()\n",
    "print(\"=\"*10 + f\" Training finished. Got best model with val_loss={best_val_loss:.5f} \" + \"=\"*10)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SjVJE1xwIf-"
   },
   "source": [
    "### 4 - Evaluate\n",
    "\n",
    "To evaluate the model performance in 3D hand pose estimation we use same metrics as in Part 1: MPJPE and PA-MPJPE.\n",
    "\n",
    "Finish the *TODOs* below to evaluate the model performance by reporting MPJPE and PA-MPJPE value. For reference, a good model should have MPJPE ~ 35mm, PA-MPJPE ~ 15mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-_hqHzuwIf-"
   },
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, device):\n",
    "    epoch_loss_3d_pos = AverageMeter()\n",
    "    epoch_loss_3d_pos_procrustes = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loader = tqdm(test_loader, dynamic_ncols=True)\n",
    "        for i, (input, pose_3d_gt, vis_flag, _) in enumerate(test_loader):\n",
    "            # Pose 3D prediction\n",
    "            input = input.to(device)\n",
    "            pose_3d_pred = model(input)\n",
    "\n",
    "            # Unnormalize predicted and GT pose 3D kpts\n",
    "            pred_3d_pts = pose_3d_pred.cpu().detach().numpy()\n",
    "            pred_3d_pts = pred_3d_pts * test_dataset.joint_std + test_dataset.joint_mean\n",
    "            gt_3d_kpts = pose_3d_gt.cpu().detach().numpy()\n",
    "            gt_3d_kpts = gt_3d_kpts * test_dataset.joint_std + test_dataset.joint_mean\n",
    "\n",
    "            # Filter out invalid joints\n",
    "            valid_pred_3d_kpts = torch.from_numpy(pred_3d_pts)\n",
    "            valid_pred_3d_kpts = valid_pred_3d_kpts[vis_flag].view(1, -1, 3)\n",
    "            valid_pose_3d_gt = torch.from_numpy(gt_3d_kpts)\n",
    "            valid_pose_3d_gt = valid_pose_3d_gt[vis_flag].view(1, -1, 3)\n",
    "            # Compute MPJPE\n",
    "            epoch_loss_3d_pos.update(mpjpe(valid_pred_3d_kpts, valid_pose_3d_gt).item(), 1)\n",
    "            epoch_loss_3d_pos_procrustes.update(p_mpjpe(valid_pred_3d_kpts, valid_pose_3d_gt), 1)\n",
    "\n",
    "    return epoch_loss_3d_pos.avg, epoch_loss_3d_pos_procrustes.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvWbBrYJwIf-"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize model, device and load in pretrained weight. Remember to set model in eval() mode\n",
    "model = ...\n",
    "device = ...\n",
    "load_pretrained_weights(model, torch.load(\"REPLACE_TO_BE_MODEL_CKPT_PATH\", map_location=device))\n",
    "\n",
    "# Evalute model performance on test set\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "mpjpe_, pa_mpjpe_ = evaluate(test_loader, model, device)\n",
    "print(f\"Model performance on test set: MPJPE: {mpjpe_:.2f} (mm) PA-MPJPE: {pa_mpjpe_:.2f} (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVneRTxSwIgA"
   },
   "source": [
    "We can also visualize the ground truth 3D hand pose and predicted 3D hand pose to get better qualitative evaluation. Finish the *TODOs* below to generate the comparison plot. Select three random data samples and attach the comparison plot (GT on left, pred on right) in your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuhoXrrQwIgA"
   },
   "outputs": [],
   "source": [
    "# TODO: Select random idx\n",
    "vis_idx = ...\n",
    "input, pose_3d_gt, vis_flag, _ = test_dataset[vis_idx]\n",
    "\n",
    "# Visualize ground truth 3D hand kpts\n",
    "gt_3d = test_dataset.inv_normalize_3d(pose_3d_gt.numpy())\n",
    "gt_3d[~vis_flag] = None\n",
    "vis_data_3d(gt_3d, title=f\"GT - idx={vis_idx}\")\n",
    "\n",
    "# TODO: Visualize predicted 3D hand kpts\n",
    "pred_kpts_3d = ...\n",
    "pred_kpts_3d[~vis_flag] = None\n",
    "vis_data_3d(pred_kpts_3d, title=f\"Pred - idx={vis_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA56m8W4wIgC"
   },
   "source": [
    "### 5 - Submission\n",
    "\n",
    "Please submit materials specified below to Gradescope.\n",
    "\n",
    "- Finished Notebook `CIS_5810_Project_8_2.ipynb`\n",
    "- Finished `model/potter.py`\n",
    "- Single PDF file, including:\n",
    "    - Training and validation loss curve plot from wandb\n",
    "    - MPJPE (mm) and PA-MPJPE (mm)\n",
    "    - Three visualization plots, with GT on left and Pred on right"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "potter_pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
