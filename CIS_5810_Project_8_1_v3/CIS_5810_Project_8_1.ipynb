{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YclQcF4Qsy2v"
   },
   "source": [
    "## **CIS 5810 Project 8 - Part I - Hand Pose Estimation with Transformer**\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In Project 7 we follow a top-down method to perform 2D hand pose estimation from a single RGB image, using one of the most popular CNN based network - UNet. In this project, we will extend the hand pose estimation task to 3D with the focus on building a Transformer based network, which is the core design of many state-of-the-art methods.\n",
    "\n",
    "**Lifting 2D to 3D hand pose estimation**\n",
    "\n",
    "The task is to lift 2D hand pose to get the 3D hand pose. In other words, the input to our model is the 2D hand keypoints $P (J,2)$ and the output is 3D hand keypoints $\\hat{P}(J,3)$ in camera coordinate system. Since there is no image input, the training time would be greatly reduced in this project.\n",
    "\n",
    "**What to do**\n",
    "\n",
    "All parts that need your implementation are marked as *TODOs*, including model architecture build, training pipeline and final inference & evaluation. The file structure and helper function detail are listed below.\n",
    "\n",
    "- `imgs/`: directory where sample images for display are stored.\n",
    "- `dataset/`\n",
    "    - `dataset.py`: main Dataset to load and preprocess Ego-Exo4D data for model training.\n",
    "    - `data_vis.py`: some helper functions to visualize 2D & 3D hand kepypoints.\n",
    "- `utils/`\n",
    "    - `utils.py`: a list of utility functions to help model training and debugging.\n",
    "    - `loss.py`: implementation of loss function and metrics to evaluate model performance.\n",
    "- `model/`\n",
    "    - `model.py`: Implementation of PoseTransformer, which is the model for pose estimation\n",
    "- `CIS_5810_Project_8-1.ipynb`: intergrate every parts from above together from loading dataset to model training and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZU9C5HXsy2x"
   },
   "source": [
    "### 0 - Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQHBFqbvsy2y"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from easydict import EasyDict as edict\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.dataset import ego4dDataset\n",
    "from dataset.data_vis import *\n",
    "from model.model import PoseTransformer\n",
    "from utils.utils import *\n",
    "from utils.loss import Pose3DLoss, mpjpe, p_mpjpe\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okt-uLy-sy2z"
   },
   "source": [
    "#### 1 - Load Ego4D Dataset\n",
    "\n",
    "Download the data from [here](https://drive.google.com/drive/folders/1g9SpvjDyndOGQg70AqqkKimZRPwvgV5_?usp=sharing) and put it under `{anno_dir}` as shown below.\n",
    "\n",
    "```\n",
    "{anno_dir}\n",
    "    ├── ego_pose_gt_anno_test.json\n",
    "    ├── ego_pose_gt_anno_train.json\n",
    "    └── ego_pose_gt_anno_val.json\n",
    "```\n",
    "\n",
    "The JSON file is similar to the one you used from Project 7 but with additional 3D hand poses annotation. Run cell below to gain better idea about the dataset.\n",
    "\n",
    "*NOTE: Ego-Exo4D dataset annotation is still in progress, thus there might be some bad/missing annotations. However, the general quality of the dataset should be good enough for you to train a hand pose estimation model that gives reasonable predictions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p13f3tussy2z"
   },
   "outputs": [],
   "source": [
    "# TODO: Modify data_root_path as {anno_dir}\n",
    "data_root_path = ...\n",
    "\n",
    "# Since there is only hand keypoints data, we simply just transform it to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# TODO: Initialize the train, val and test Dataset\n",
    "# Hint: take a look at the implementation of ego4dDataset for initialization requirement\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "test_dataset = ...\n",
    "\n",
    "# Check the dataset length\n",
    "print(\"Train: \", len(train_dataset))\n",
    "print(\"Val: \", len(val_dataset))\n",
    "print(\"Test: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfh4Rcxosy20"
   },
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "We follow the same hand joints index distribution as in project 7 (See figure below). The relationship between 2D and 3D hand keypoints can be defined as\n",
    "\n",
    "$$\\lambda \\begin{bmatrix}\n",
    "u\\\\\n",
    "v\\\\\n",
    "1\n",
    "\\end{bmatrix} = K \\begin{bmatrix}\n",
    "X_c\\\\\n",
    "Y_c\\\\\n",
    "Z_c\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $[u,v]^{T}$ are the 2D hand keypoints on image plane, $[X_c, Y_c, Z_c]^{T}$ are 3D hand keypoints in camera coodinate system, $\\lambda$ is the scale factor, and $K$ is the camera intrinsic matrix. If you take a look at the implementation of the provided `ego4dDataset` dataset, there are two preprocessing we have done to help training:\n",
    "\n",
    "1. The 3D hand keypoints are offseted by hand wrist s.t. $\\hat{P}[0]=[0,0]$. Recovering the 3D coordinates from 2D coordinates is not a trivial problem as the scale information($\\lambda$) is unknown. Therefore, we are not predicting the real 3D hand pose in the camera coordinate system, but rather the relative location of each hand joint w.r.t the hand wrist.\n",
    "\n",
    "2. The 2D and offseted 3D hand keypoints are then normalized by subtracting the mean and then divided by stand deviation, similar to what we did in project 7 (on preprocess images) to help stablize model training and convergence.\n",
    "\n",
    "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"imgs/hand_index.png\" alt=\"Hand index\" width =\"300\" height=\"350\">\n",
    "        <figcaption>Figure 1: Hand joints index</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "**Dataset items**\n",
    "\n",
    "If you take a look at the dataset returnd data it has four objects: `kpts_2d`, `kpts_3d`, `weight`, `metadata`. `kpts_2d` and `kpts_3d` corresponds to the processed 2D and 3D hand keypoints; `metadata` contains frame number and take info for quick debugging; `weight` is a boolean array indicating whether each joint is valid or not. If `False`, then this joint won't been included during model training.\n",
    "\n",
    "\n",
    "In cell below, you can select data from a different dataset and visualize those ground truth pose. You can even try visualize the 2D hand keypoints by overlaying it on the real images from project 7 to get a better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCGaJQTisy20"
   },
   "outputs": [],
   "source": [
    "## TODO: Modify as needed to take a look at the dataset\n",
    "check_dataset = ...\n",
    "idx = ...\n",
    "\n",
    "## Get one sample for visualization\n",
    "kpts_2d, kpts_3d, weight, metadata = check_dataset[idx]\n",
    "\n",
    "# Visualize 2D hand kpts in image plane\n",
    "kpts_2d = check_dataset.inv_normalize_2d(kpts_2d.numpy())\n",
    "# Assign None to invalid kpts (so it won't be displayed)\n",
    "kpts_2d[~weight] = None\n",
    "vis_data_2d(kpts_2d, title=\"GT 2D\")\n",
    "\n",
    "# Visualize 3D hand kpts in camera coordinate system\n",
    "# Hint: Change the 3D plot view angle to get a better visualization:\n",
    "# https://matplotlib.org/stable/api/toolkits/mplot3d/view_angles.html\n",
    "gt_3d = check_dataset.inv_normalize_3d(kpts_3d.numpy())\n",
    "# Assign None to invalid kpts (so it won't be displayed)\n",
    "gt_3d[~weight] = None\n",
    "vis_data_3d(gt_3d, title=\"GT 3D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbGLA_LFsy20"
   },
   "source": [
    "### 2 - Define model\n",
    "\n",
    "**General overview**\n",
    "\n",
    "The 2D to 3D hand pose lifting model is based on Transformer with architecture shown in Figure 2. It has a similar design as the [ViT](https://arxiv.org/pdf/2010.11929.pdf), but takes 2D hand keypoints $(21,2)$ as input and treat each joint as a patch. Each 2D keypoints path is first projected into embedding space via linear layer $(21,D)$, and then added along with positional encoding to retain positional information. The resulting emebedded feature vectors are then fed into Transformer Encoder, which consists of alternating layers of multihead self-attention (MSA), MLP layer and layer normalization (LN). The same encoder is repeated several times and the output of final encoder block is then fed into the MLP head, which consists of layer normalization and a single linear layer, to project embedded vectors $(21,D)$ back to 3D coordinates $(21,3)$ as the final model output.\n",
    "\n",
    "<div style=\"display: center; justify-content: space-between; text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"imgs/model.png\" alt=\"Model architecture\" width =\"700\" height=\"530\">\n",
    "        <figcaption>Figure 2: Model architecture</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "**Multihead Self-Attention (MSA)**\n",
    "\n",
    "The core design of the Transformer is the multihead self-attention mechanism. For a single head, given an input sequence $z \\in \\mathbb{R}^{(N,D)}$, we first compute three feature vectors $q,k,v \\in \\mathbb{R}^{N \\times D_h}$, namely Query, Key and Value, via linear projection s.t.\n",
    "\n",
    "$$[q,k,v] = zU_{qkv} \\quad\\quad\\quad U_{qkv} \\in \\mathbb{R}^{D \\times 3D_h}$$\n",
    "\n",
    "Then compute the attention weight $A$, where $A_{ij}$ indicates the pairwise similarity between two elements $z_i$ and $z_j$ computed based on their respective query $q_i$ and value $k_i$:\n",
    "\n",
    "$$A = softmax(\\frac{qk^T}{\\sqrt{D_h}}) \\quad\\quad\\quad A \\in \\mathbb{R}^{N \\times N}$$\n",
    "\n",
    "Then we compute the weighted sum over all values $v$ in the sequence via self-attention (SA):\n",
    "\n",
    "$$SA(z)=Av$$\n",
    "\n",
    "Multihead self-attention (MSA) extends the self-attention (SA) in which we run $k$ self-attention operations, which are called *head*, in parallel and then project the concatenated head output as current MSA output. In this case, the head embedding dimension $D_h$ is defined as $\\frac{D}{k}$ s.t. the concatenated vectors maintain the same embedding dimension $D$.\n",
    "\n",
    "$$MSA(z) = [SA_1(z), SA_2(z), ..., SA_k(z)]U_{msa} \\quad\\quad\\quad U_{msa} \\in \\mathbb{R}^{D \\times D}$$\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Your task is to finish the implementation of `forward()` method of Encoder block and Attention block, and part of the main model in `model/model.py` (skeleton code has been given), and the training and evaluation pipeline below within this notebook which are all marked as *TODO*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_OglhBksy20"
   },
   "source": [
    "### 3 - Train model\n",
    "\n",
    "Define the pipeline of model training and validation similar to project 7. See below TODOs to fill in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2C9jMcMsy20"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    total_loss = AverageMeter()\n",
    "\n",
    "    # TODO: set model to training mode\n",
    "\n",
    "\n",
    "    train_loader = tqdm(train_loader, dynamic_ncols=True)\n",
    "    print_interval = len(train_loader) // 6\n",
    "    # Iterate over all training samples\n",
    "    for i, (kpts_2d, gt_kpts_3d, weight, _) in enumerate(train_loader):\n",
    "        # TODO:\n",
    "        # 1. Put all revelant data onto same device\n",
    "        # 2. Model forward (given 2d kpts and weight, return 3d kpts)\n",
    "\n",
    "\n",
    "        # TODO: Compute loss (remember to pass weight)\n",
    "        loss = ...\n",
    "        # Record current batch's loss\n",
    "        total_loss.update(loss.item())\n",
    "\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Clear the old parameter gradients\n",
    "        # 2. Compute the derivative of loss w.r.t the model parameters\n",
    "        # 3. Update the model parameters with optimizer\n",
    "\n",
    "\n",
    "        # Log loss to wandb\n",
    "        if (i+1) % print_interval == 0:\n",
    "            wandb.log({\"Loss/train\": total_loss.avg})\n",
    "\n",
    "    return total_loss.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, device):\n",
    "    total_loss = AverageMeter()\n",
    "\n",
    "    # TODO: set model to evaluate mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loader = tqdm(val_loader, dynamic_ncols=True)\n",
    "        # Iterate over all validation samples\n",
    "        for _, (kpts_2d, gt_kpts_3d, weight, _) in enumerate(val_loader):\n",
    "            # TODO:\n",
    "            # 1. Put all revelant data onto same device\n",
    "            # 2. Model forward (given 2d kpts and weight, return 3d kpts)\n",
    "\n",
    "\n",
    "            # TODO: Compute loss (remember to pass weight)\n",
    "            loss = ...\n",
    "            # Record current batch's loss\n",
    "            total_loss.update(loss.item())\n",
    "\n",
    "        # Log loss to wandb\n",
    "        wandb.log({\"Loss/val\": total_loss.avg})\n",
    "\n",
    "    return total_loss.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mpW0T6bsy21"
   },
   "source": [
    "With the training and validation pipeline set up, we can then instantiate our model and define other essential parts for model training including:\n",
    "\n",
    "- **optimizer:** We will be using Adam optimizer with learning rate 2e-4.\n",
    "\n",
    "- **loss function (criterion):** Use Pose3DLoss() as the loss function, which computes the MSE between predicted and ground truth 3D hand keypoints with filtering. The implementation has been given, please take a look at it before use.\n",
    "\n",
    "- **dataloaders**: train and val dataloader to return data in batches. Default batch size is 64, and you can adjust properly as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pY1RAuksy21"
   },
   "outputs": [],
   "source": [
    "# TODO: Instantiate the model and define device for training to use GPU if available\n",
    "device = ...\n",
    "model = ...\n",
    "\n",
    "# TODO: Define criterion\n",
    "criterion = ...\n",
    "# TODO: Define optimizer\n",
    "optimizer = ...\n",
    "\n",
    "# Define train and val dataloader\n",
    "train_loader = ...\n",
    "val_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqdhpIYysy21"
   },
   "source": [
    "Finally, we can start to train the model. We will log training and validation loss to wandb() to monitor the model training status. Run cell below to initialization wandb and start training. For Wandb Usage, please refer to [Wandb Github](https://github.com/wandb/wandb?tab=readme-ov-file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7zQ93Pesy21"
   },
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "# TODO: Define current run name\n",
    "current_run_name = ...\n",
    "wandb.init(project=\"CIS5810_project_8_1\", name=current_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq8kPPkQsy22"
   },
   "outputs": [],
   "source": [
    "# Define output directory; modify as needed (where model ckpt will be saved)\n",
    "output_root = \"output\"\n",
    "output_dir = os.path.join(output_root, current_run_name)\n",
    "print(\"=\"*10 + f\" Training started. Output will be saved at {output_dir} \" + \"=\"*10)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Default training epoches and best val loss\n",
    "epochs = 30\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"=\"*10, f\"Epoch [{epoch}/{epochs}]\", \"=\"*10)\n",
    "    # train for one epoch\n",
    "    _ = train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss = validate(val_loader, model, criterion, epoch, device)\n",
    "\n",
    "    # Save best model weight\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save model weight\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, f\"final_state.pth.tar\"))\n",
    "        print(f\"Saving model weight with best val_loss={val_loss:.5f}\")\n",
    "    print()\n",
    "print(\"=\"*10 + f\" Training finished. Got best model with val_loss={best_val_loss:.5f} \" + \"=\"*10)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIQVVlT3sy22"
   },
   "source": [
    "### 4 - Evaluate\n",
    "\n",
    "To evaluate the model performance in lifting 2D to 3D hand pose estimation we use two metrics: MPJPE and PA-MPJPE. Mean Per-Joints Position Error (MPJPE) computes the average distance between each predicted joint and ground truth joint. PA-MPJPE performs procruste aligns on predicted pose first and then calculate MPJPE, thus its value is lower (better) and corresponds to the model performance upper bound.\n",
    "\n",
    "Finish the *TODOs* below to evaluate the model performance by reporting MPJPE and PA-MPJPE value. For reference, a good model should have MPJPE ~ 25mm, PA-MPJPE ~ 10mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa1aSfyDsy22"
   },
   "outputs": [],
   "source": [
    "def evaluate(testset, model, device):\n",
    "    epoch_loss_3d_pos = AverageMeter()\n",
    "    epoch_loss_3d_pos_procrustes = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for kpts_2d, gt_kpts_3d, weight, _ in tqdm(testset, total=len(testset)):\n",
    "            # Pose 3D prediction\n",
    "            kpts_2d = kpts_2d.unsqueeze(0).to(device)\n",
    "            weight = weight.unsqueeze(0).to(device)\n",
    "            gt_kpts_3d = gt_kpts_3d.unsqueeze(0).to(device)\n",
    "            pred_kpts_3d = model(kpts_2d, weight)\n",
    "\n",
    "            # mm to m\n",
    "            pred_kpts_3d *= 1000.0\n",
    "            gt_kpts_3d *= 1000.0\n",
    "\n",
    "            # Get valid kpts\n",
    "            valid_pred_kpts_3d = pred_kpts_3d[weight].view(1,-1,3).cpu().detach().numpy()\n",
    "            valid_gt_kpts_3d = gt_kpts_3d[weight].view(1,-1,3).cpu().detach().numpy()\n",
    "            # Un-normalize\n",
    "            valid_pred_kpts_3d = testset.inv_normalize_3d(valid_pred_kpts_3d)\n",
    "            valid_gt_kpts_3d = testset.inv_normalize_3d(valid_gt_kpts_3d)\n",
    "            # Compute MPJPE\n",
    "            epoch_loss_3d_pos.update(mpjpe(valid_pred_kpts_3d, valid_gt_kpts_3d).item())\n",
    "            epoch_loss_3d_pos_procrustes.update(p_mpjpe(valid_pred_kpts_3d, valid_gt_kpts_3d).item())\n",
    "\n",
    "    return epoch_loss_3d_pos.avg, epoch_loss_3d_pos_procrustes.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VUVtS9Ysy22"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize model, device and load in pretrained weight. Remember to set model in eval() mode\n",
    "model = ...\n",
    "\n",
    "# Evalute model performance\n",
    "mpjpe_, pa_mpjpe_ = evaluate(test_dataset, model, device)\n",
    "print(f\"Model performance on test set: MPJPE: {mpjpe_:.2f} (mm) PA-MPJPE: {pa_mpjpe_:.2f} (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSkGiR2qsy22"
   },
   "source": [
    "We can also visualize the ground truth 3D hand pose and predicted 3D hand pose to get better qualitative evaluation. Finish the *TODOs* below to generate the comparison plot. **Select three random data samples and attach the comparison plot (GT on left, pred on right) in your final submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpUbM7fIsy23"
   },
   "outputs": [],
   "source": [
    "# TODO: Select random idx\n",
    "vis_idx = ...\n",
    "kpts_2d, gt_kpts_3d, weight, _ = test_dataset[vis_idx]\n",
    "\n",
    "# Visualize ground truth 3D hand kpts\n",
    "gt_3d = check_dataset.inv_normalize_3d(gt_kpts_3d.numpy())\n",
    "gt_3d[~weight] = None\n",
    "vis_data_3d(gt_3d, title=f\"GT - idx={vis_idx}\")\n",
    "\n",
    "# TODO: Visualize predicted 3D hand kpts\n",
    "pred_kpts_3d = ...\n",
    "vis_data_3d(pred_kpts_3d, title=f\"Pred - idx={vis_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjAlrCAmsy23"
   },
   "source": [
    "### 5 - Submission\n",
    "\n",
    "Please submit materials specified below to Gradescope.\n",
    "\n",
    "- Finished Notebook `CIS_5810_Project_8-1.ipynb`\n",
    "- Finished `model.py`\n",
    "- Single PDF file, including:\n",
    "    - Training and validation loss curve plot from wandb\n",
    "    - Three comparison plots\n",
    "    - Report Model performance on test set: MPJPE and PA-MPJPE."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
